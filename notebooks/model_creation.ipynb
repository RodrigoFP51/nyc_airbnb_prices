{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3473a182",
   "metadata": {},
   "source": [
    "## NYC Airbnb Price Prediction\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354e7180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from feature_engine.encoding import RareLabelEncoder\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder, PowerTransformer, MinMaxScaler\n",
    ")\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import (\n",
    "    KFold, RandomizedSearchCV, train_test_split\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_log_error, r2_score\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9beff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data/airbnb_imputed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11252d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0e40df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='object').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd875c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e7bfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f599bef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.query(\"price > 0 & price < 800\")\n",
    "#df = df[df['price']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89d4a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['last_review'] = pd.to_datetime(df['last_review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651dab9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'] = df['price'].astype(\"float64\")\n",
    "df['name'] = df['name'].astype(object)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc74572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df.last_review.dt.year\n",
    "df['month'] = df.last_review.dt.month\n",
    "\n",
    "df[['year','month']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c0d636",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['price', 'host_id', 'host_name', 'id', 'last_review'],axis = 1)\n",
    "y = np.log(df['price'])\n",
    "\n",
    "num_cols = X.select_dtypes(include = np.number).columns.to_list()\n",
    "cat_cols = X.select_dtypes(exclude = np.number).drop('name', axis=1).columns.to_list()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "print(X_train[num_cols].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229579da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "pipe_cat = Pipeline(\n",
    "    steps = [\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe_num = Pipeline(\n",
    "    steps = [\n",
    "    ('scale', MinMaxScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe_text = Pipeline(\n",
    "    steps = [\n",
    "        ('text_vec', CountVectorizer(analyzer='word',\n",
    "                                     stop_words='english',\n",
    "                                     max_features=30))\n",
    "    ]\n",
    ")\n",
    "\n",
    "#pd.DataFrame(pipe_text.fit_transform(X_train['name']).toarray(), columns=pipe_text.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931fcfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers = [\n",
    "    #('num', pipe_num, num_cols),\n",
    "    ('text', pipe_text, 'name'),\n",
    "    ('cat', pipe_cat, cat_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "pd.DataFrame(preprocess.fit_transform(X_train), columns=preprocess.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "074efe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Pipeline(steps = [\n",
    "    ('rare', RareLabelEncoder(tol=0.03, variables=['neighbourhood'])),\n",
    "    ('preprocessor', preprocess),\n",
    "    ('lgbm', LGBMRegressor())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5261a55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002877 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1511\n",
      "[LightGBM] [Info] Number of data points in the train set: 23711, number of used features: 58\n",
      "[LightGBM] [Info] Start training from score 4.704162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6583243034578063"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fit = model.fit(X_train, y_train) \n",
    "model_fit.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf107b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_fit.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179c7f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RMSLE for LGBM: {np.sqrt(mean_squared_log_error(y_test, y_pred))},\\\n",
    "\\nR2 for LGBM: {r2_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698431fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingRegressor, AdaBoostRegressor\n",
    ")\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "models = [\n",
    "    ('huber', HuberRegressor()),\n",
    "    ('gb', GradientBoostingRegressor()),\n",
    "    ('ada', AdaBoostRegressor()),\n",
    "    ('xgb', XGBRegressor())\n",
    "] \n",
    "\n",
    "for name, model in models:\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('rare', RareLabelEncoder(tol=0.03, variables=['neighbourhood'])),\n",
    "        ('preprocessor', preprocess),\n",
    "        (name, model)\n",
    "    ])\n",
    "    \n",
    "    pipe.fit(X_train,y_train)\n",
    "    preds = pipe.predict(X_test)\n",
    "    print(f\"\"\"{name} R2: {r2_score(y_test, preds)}\n",
    "          {name} RMSLE: {np.sqrt(mean_squared_log_error(y_test, preds))}\"\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0279e825",
   "metadata": {},
   "source": [
    "## Otimização dos parâmetros do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4955b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_grid = {\n",
    "    'lgbm__num_leaves': [7, 14, 21],\n",
    "    'lgbm__learning_rate': [0.1, 0.03, 0.001],\n",
    "    'lgbm__max_depth': [-1, 3, 5],\n",
    "    'lgbm__n_estimators': [200, 500, 1000],\n",
    "    'preprocessor__text__text_vec__max_features': [10,20,50,150]\n",
    "}\n",
    "\n",
    "model.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2762de79",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_tune = RandomizedSearchCV(\n",
    "    model, \n",
    "    param_distributions = lgbm_grid,\n",
    "    cv = 5,\n",
    "    scoring = 'neg_mean_squared_log_error',\n",
    "    return_train_score = True,\n",
    "    n_iter = 10,\n",
    "    verbose = 1,\n",
    "    random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ec1cd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004219 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1446\n",
      "[LightGBM] [Info] Number of data points in the train set: 18968, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.702090\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004748 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1446\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.703432\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002185 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1446\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.705202\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002031 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1445\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.705213\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002915 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1446\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.704875\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002832 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1446\n",
      "[LightGBM] [Info] Number of data points in the train set: 18968, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.702090\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1446\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.703432\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1446\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.705202\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002486 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1445\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.705213\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003238 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1446\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.704875\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003529 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1471\n",
      "[LightGBM] [Info] Number of data points in the train set: 18968, number of used features: 48\n",
      "[LightGBM] [Info] Start training from score 4.702090\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005903 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1473\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 48\n",
      "[LightGBM] [Info] Start training from score 4.703432\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002493 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1473\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 48\n",
      "[LightGBM] [Info] Start training from score 4.705202\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004072 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1472\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 48\n",
      "[LightGBM] [Info] Start training from score 4.705213\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003843 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1473\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 48\n",
      "[LightGBM] [Info] Start training from score 4.704875\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004116 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1471\n",
      "[LightGBM] [Info] Number of data points in the train set: 18968, number of used features: 48\n",
      "[LightGBM] [Info] Start training from score 4.702090\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004113 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1473\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 48\n",
      "[LightGBM] [Info] Start training from score 4.703432\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004913 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1473\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 48\n",
      "[LightGBM] [Info] Start training from score 4.705202\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004143 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1472\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 48\n",
      "[LightGBM] [Info] Start training from score 4.705213\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003461 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1473\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 48\n",
      "[LightGBM] [Info] Start training from score 4.704875\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002613 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1446\n",
      "[LightGBM] [Info] Number of data points in the train set: 18968, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.702090\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002965 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1446\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.703432\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002897 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1446\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.705202\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1445\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.705213\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003026 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1446\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.704875\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007263 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1550\n",
      "[LightGBM] [Info] Number of data points in the train set: 18968, number of used features: 78\n",
      "[LightGBM] [Info] Start training from score 4.702090\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006882 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1553\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 78\n",
      "[LightGBM] [Info] Start training from score 4.703432\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006149 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1554\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 78\n",
      "[LightGBM] [Info] Start training from score 4.705202\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013163 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1551\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 78\n",
      "[LightGBM] [Info] Start training from score 4.705213\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007597 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1554\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 78\n",
      "[LightGBM] [Info] Start training from score 4.704875\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015399 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1774\n",
      "[LightGBM] [Info] Number of data points in the train set: 18968, number of used features: 178\n",
      "[LightGBM] [Info] Start training from score 4.702090\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011850 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1775\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 178\n",
      "[LightGBM] [Info] Start training from score 4.703432\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016753 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1780\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 178\n",
      "[LightGBM] [Info] Start training from score 4.705202\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012005 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1775\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 178\n",
      "[LightGBM] [Info] Start training from score 4.705213\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011347 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1779\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 178\n",
      "[LightGBM] [Info] Start training from score 4.704875\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004686 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1446\n",
      "[LightGBM] [Info] Number of data points in the train set: 18968, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.702090\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002781 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1446\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.703432\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002535 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1446\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.705202\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003024 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1445\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.705213\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003270 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1446\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.704875\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006546 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1550\n",
      "[LightGBM] [Info] Number of data points in the train set: 18968, number of used features: 78\n",
      "[LightGBM] [Info] Start training from score 4.702090\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1553\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 78\n",
      "[LightGBM] [Info] Start training from score 4.703432\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006813 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1554\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 78\n",
      "[LightGBM] [Info] Start training from score 4.705202\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007170 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1551\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 78\n",
      "[LightGBM] [Info] Start training from score 4.705213\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007307 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1554\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 78\n",
      "[LightGBM] [Info] Start training from score 4.704875\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002855 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1446\n",
      "[LightGBM] [Info] Number of data points in the train set: 18968, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.702090\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003787 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1446\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.703432\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003625 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1446\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.705202\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1445\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.705213\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002226 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1446\n",
      "[LightGBM] [Info] Number of data points in the train set: 18969, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.704875\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005323 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1562\n",
      "[LightGBM] [Info] Number of data points in the train set: 23711, number of used features: 78\n",
      "[LightGBM] [Info] Start training from score 4.704162\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[(&#x27;rare&#x27;,\n",
       "                                              RareLabelEncoder(tol=0.03,\n",
       "                                                               variables=[&#x27;neighbourhood&#x27;])),\n",
       "                                             (&#x27;preprocessor&#x27;,\n",
       "                                              ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                                                transformers=[(&#x27;text&#x27;,\n",
       "                                                                               Pipeline(steps=[(&#x27;text_vec&#x27;,\n",
       "                                                                                                CountVectorizer(max_features=30,\n",
       "                                                                                                                stop_words=&#x27;english&#x27;))]),\n",
       "                                                                               &#x27;name&#x27;),\n",
       "                                                                              (&#x27;cat&#x27;,\n",
       "                                                                               Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                                                                OneHotEncoder(handle_unknown=...\n",
       "                                                                                &#x27;neighbourhood&#x27;,\n",
       "                                                                                &#x27;room_type&#x27;])])),\n",
       "                                             (&#x27;lgbm&#x27;, LGBMRegressor())]),\n",
       "                   param_distributions={&#x27;lgbm__learning_rate&#x27;: [0.1, 0.03,\n",
       "                                                                0.001],\n",
       "                                        &#x27;lgbm__max_depth&#x27;: [-1, 3, 5],\n",
       "                                        &#x27;lgbm__n_estimators&#x27;: [200, 500, 1000],\n",
       "                                        &#x27;lgbm__num_leaves&#x27;: [7, 14, 21],\n",
       "                                        &#x27;preprocessor__text__text_vec__max_features&#x27;: [10,\n",
       "                                                                                       20,\n",
       "                                                                                       50,\n",
       "                                                                                       150]},\n",
       "                   random_state=42, return_train_score=True,\n",
       "                   scoring=&#x27;neg_mean_squared_log_error&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[(&#x27;rare&#x27;,\n",
       "                                              RareLabelEncoder(tol=0.03,\n",
       "                                                               variables=[&#x27;neighbourhood&#x27;])),\n",
       "                                             (&#x27;preprocessor&#x27;,\n",
       "                                              ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                                                transformers=[(&#x27;text&#x27;,\n",
       "                                                                               Pipeline(steps=[(&#x27;text_vec&#x27;,\n",
       "                                                                                                CountVectorizer(max_features=30,\n",
       "                                                                                                                stop_words=&#x27;english&#x27;))]),\n",
       "                                                                               &#x27;name&#x27;),\n",
       "                                                                              (&#x27;cat&#x27;,\n",
       "                                                                               Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                                                                OneHotEncoder(handle_unknown=...\n",
       "                                                                                &#x27;neighbourhood&#x27;,\n",
       "                                                                                &#x27;room_type&#x27;])])),\n",
       "                                             (&#x27;lgbm&#x27;, LGBMRegressor())]),\n",
       "                   param_distributions={&#x27;lgbm__learning_rate&#x27;: [0.1, 0.03,\n",
       "                                                                0.001],\n",
       "                                        &#x27;lgbm__max_depth&#x27;: [-1, 3, 5],\n",
       "                                        &#x27;lgbm__n_estimators&#x27;: [200, 500, 1000],\n",
       "                                        &#x27;lgbm__num_leaves&#x27;: [7, 14, 21],\n",
       "                                        &#x27;preprocessor__text__text_vec__max_features&#x27;: [10,\n",
       "                                                                                       20,\n",
       "                                                                                       50,\n",
       "                                                                                       150]},\n",
       "                   random_state=42, return_train_score=True,\n",
       "                   scoring=&#x27;neg_mean_squared_log_error&#x27;, verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;rare&#x27;,\n",
       "                 RareLabelEncoder(tol=0.03, variables=[&#x27;neighbourhood&#x27;])),\n",
       "                (&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;text&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;text_vec&#x27;,\n",
       "                                                                   CountVectorizer(max_features=30,\n",
       "                                                                                   stop_words=&#x27;english&#x27;))]),\n",
       "                                                  &#x27;name&#x27;),\n",
       "                                                 (&#x27;cat&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                                 sparse_output=False))]),\n",
       "                                                  [&#x27;neighbourhood_group&#x27;,\n",
       "                                                   &#x27;neighbourhood&#x27;,\n",
       "                                                   &#x27;room_type&#x27;])])),\n",
       "                (&#x27;lgbm&#x27;, LGBMRegressor())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RareLabelEncoder</label><div class=\"sk-toggleable__content\"><pre>RareLabelEncoder(tol=0.03, variables=[&#x27;neighbourhood&#x27;])</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;text&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;text_vec&#x27;,\n",
       "                                                  CountVectorizer(max_features=30,\n",
       "                                                                  stop_words=&#x27;english&#x27;))]),\n",
       "                                 &#x27;name&#x27;),\n",
       "                                (&#x27;cat&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                sparse_output=False))]),\n",
       "                                 [&#x27;neighbourhood_group&#x27;, &#x27;neighbourhood&#x27;,\n",
       "                                  &#x27;room_type&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">text</label><div class=\"sk-toggleable__content\"><pre>name</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(max_features=30, stop_words=&#x27;english&#x27;)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cat</label><div class=\"sk-toggleable__content\"><pre>[&#x27;neighbourhood_group&#x27;, &#x27;neighbourhood&#x27;, &#x27;room_type&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;, sparse_output=False)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">remainder</label><div class=\"sk-toggleable__content\"><pre>[&#x27;latitude&#x27;, &#x27;longitude&#x27;, &#x27;minimum_nights&#x27;, &#x27;number_of_reviews&#x27;, &#x27;reviews_per_month&#x27;, &#x27;calculated_host_listings_count&#x27;, &#x27;availability_365&#x27;, &#x27;year&#x27;, &#x27;month&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('rare',\n",
       "                                              RareLabelEncoder(tol=0.03,\n",
       "                                                               variables=['neighbourhood'])),\n",
       "                                             ('preprocessor',\n",
       "                                              ColumnTransformer(remainder='passthrough',\n",
       "                                                                transformers=[('text',\n",
       "                                                                               Pipeline(steps=[('text_vec',\n",
       "                                                                                                CountVectorizer(max_features=30,\n",
       "                                                                                                                stop_words='english'))]),\n",
       "                                                                               'name'),\n",
       "                                                                              ('cat',\n",
       "                                                                               Pipeline(steps=[('encoder',\n",
       "                                                                                                OneHotEncoder(handle_unknown=...\n",
       "                                                                                'neighbourhood',\n",
       "                                                                                'room_type'])])),\n",
       "                                             ('lgbm', LGBMRegressor())]),\n",
       "                   param_distributions={'lgbm__learning_rate': [0.1, 0.03,\n",
       "                                                                0.001],\n",
       "                                        'lgbm__max_depth': [-1, 3, 5],\n",
       "                                        'lgbm__n_estimators': [200, 500, 1000],\n",
       "                                        'lgbm__num_leaves': [7, 14, 21],\n",
       "                                        'preprocessor__text__text_vec__max_features': [10,\n",
       "                                                                                       20,\n",
       "                                                                                       50,\n",
       "                                                                                       150]},\n",
       "                   random_state=42, return_train_score=True,\n",
       "                   scoring='neg_mean_squared_log_error', verbose=1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_tune.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a129ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/best_lgbm.pkl']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "best_lgbm = lgbm_tune.best_estimator_\n",
    "dump(best_lgbm, '../models/best_lgbm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d25d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_tune.best_score_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
